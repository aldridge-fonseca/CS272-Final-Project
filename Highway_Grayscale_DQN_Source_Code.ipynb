{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "SAVE_DIR = \"/content/drive/MyDrive/highway_results/grayscale_dqn\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def check_drive_connection():\n",
    "    test_file = f\"{SAVE_DIR}/.connection_test\"\n",
    "    try:\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('ok')\n",
    "        os.remove(test_file)\n",
    "        print(f\"Google Drive connected. Saving to: {SAVE_DIR}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cannot write to Google Drive! {e}\")\n",
    "        return False\n",
    "\n",
    "assert check_drive_connection(), \"Fix Google Drive connection before continuing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install highway-env stable-baselines3 gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import highway_env\n",
    "\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac14cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env():\n",
    "    env = gym.make('highway-fast-v0', render_mode='rgb_array', config={\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "    })\n",
    "    env.reset()\n",
    "    return env\n",
    "\n",
    "test_env = create_env()\n",
    "obs, _ = test_env.reset()\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, save_path, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = 0\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.current_rewards += self.locals['rewards'][0]\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_rewards.append(self.current_rewards)\n",
    "            self.current_rewards = 0\n",
    "            if len(self.episode_rewards) % 50 == 0:\n",
    "                avg = np.mean(self.episode_rewards[-50:])\n",
    "                print(f\"Episode {len(self.episode_rewards)}: Avg reward (last 50) = {avg:.2f}\")\n",
    "                np.save(f\"{self.save_path}/episode_rewards.npy\", self.episode_rewards)\n",
    "        return True\n",
    "\n",
    "\n",
    "class DownloadCallback(BaseCallback):\n",
    "    def __init__(self, save_path, download_freq=10000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.save_path = save_path\n",
    "        self.download_freq = download_freq\n",
    "        self.last_download = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps - self.last_download >= self.download_freq:\n",
    "            self.last_download = self.num_timesteps\n",
    "            try:\n",
    "                zip_name = f\"grayscale_dqn_checkpoint_{self.num_timesteps}\"\n",
    "                shutil.make_archive(f\"/content/{zip_name}\", 'zip', self.save_path)\n",
    "                files.download(f\"/content/{zip_name}.zip\")\n",
    "                print(f\"Checkpoint downloaded: {self.num_timesteps} timesteps\")\n",
    "            except Exception as e:\n",
    "                print(f\"Download skipped: {e}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 100000\n",
    "CHECKPOINT_FREQ = 10000\n",
    "\n",
    "print(\"Setting up DQN training...\")\n",
    "env = DummyVecEnv([create_env])\n",
    "\n",
    "checkpoints = glob.glob(f\"{SAVE_DIR}/checkpoint_*.zip\")\n",
    "if checkpoints:\n",
    "    latest = max(checkpoints, key=lambda x: int(x.split('_')[-2]))\n",
    "    print(f\"Resuming from: {latest}\")\n",
    "    model = DQN.load(latest, env=env, device=\"cuda\")\n",
    "    if os.path.exists(f\"{SAVE_DIR}/episode_rewards.npy\"):\n",
    "        saved_rewards = np.load(f\"{SAVE_DIR}/episode_rewards.npy\").tolist()\n",
    "    else:\n",
    "        saved_rewards = []\n",
    "else:\n",
    "    print(\"Starting new DQN training\")\n",
    "    model = DQN(\n",
    "        \"CnnPolicy\",\n",
    "        env,\n",
    "        learning_rate=5e-4,\n",
    "        buffer_size=15000,\n",
    "        learning_starts=200,\n",
    "        batch_size=32,\n",
    "        gamma=0.8,\n",
    "        train_freq=1,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=50,\n",
    "        exploration_fraction=0.7,\n",
    "        verbose=1,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    saved_rewards = []\n",
    "\n",
    "checkpoint_cb = CheckpointCallback(save_freq=CHECKPOINT_FREQ, save_path=SAVE_DIR, name_prefix=\"checkpoint\")\n",
    "reward_cb = RewardLoggerCallback(save_path=SAVE_DIR)\n",
    "reward_cb.episode_rewards = saved_rewards\n",
    "download_cb = DownloadCallback(save_path=SAVE_DIR, download_freq=CHECKPOINT_FREQ)\n",
    "\n",
    "print(f\"Training DQN for {TOTAL_TIMESTEPS} timesteps...\")\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_cb, reward_cb, download_cb])\n",
    "model.save(f\"{SAVE_DIR}/grayscale_dqn_model_3\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a025fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "rewards = reward_cb.episode_rewards\n",
    "if len(rewards) >= 50:\n",
    "    rolling = np.convolve(rewards, np.ones(50)/50, mode='valid')\n",
    "    plt.plot(np.arange(50, len(rewards)+1), rolling, 'b-', linewidth=2, label='Rolling Mean (50 ep)')\n",
    "plt.plot(rewards, 'lightblue', alpha=0.3, label='Episode Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Episodic Reward (Return)')\n",
    "plt.title('Highway-v0 GrayscaleObservation DQN - Learning Curve (ID 3)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{SAVE_DIR}/highway_grayscale_learning_curve_3.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running performance test: 500 episodes...\")\n",
    "eval_env = create_env()\n",
    "eval_rewards = []\n",
    "\n",
    "for ep in range(500):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = truncated = False\n",
    "    total = 0\n",
    "    while not (done or truncated):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = eval_env.step(action)\n",
    "        total += reward\n",
    "    eval_rewards.append(total)\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        print(f\"Completed {ep + 1}/500 | Mean: {np.mean(eval_rewards):.2f}\")\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Final mean reward: {np.mean(eval_rewards):.2f} +/- {np.std(eval_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "parts = plt.violinplot(eval_rewards, positions=[1], showmeans=True, showmedians=True)\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('steelblue')\n",
    "    pc.set_alpha(0.7)\n",
    "plt.text(1.25, np.mean(eval_rewards), f'Mean: {np.mean(eval_rewards):.2f}\\nStd: {np.std(eval_rewards):.2f}')\n",
    "plt.xlabel('Highway GrayscaleObs (DQN)')\n",
    "plt.ylabel('Episodic Reward (Return)')\n",
    "plt.title('Performance Test - 500 Episodes (ID 4)')\n",
    "plt.xticks([1], ['Grayscale DQN'])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.savefig(f\"{SAVE_DIR}/highway_grayscale_performance_4.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e713d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading final results...\")\n",
    "shutil.make_archive(\"/content/grayscale_dqn_final\", 'zip', SAVE_DIR)\n",
    "files.download(\"/content/grayscale_dqn_final.zip\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
